# Big Data Pipeline Project

## Descrição
Este projeto é uma implementação de um pipeline de dados para big data, com foco em **automação de testes** e **controle de qualidade de dados (QA/QC)**. Utilizando Python, PySpark, SQL (BigQuery) e ferramentas de testes automatizados, o objetivo é demonstrar um fluxo completo de ETL (Extração, Transformação e Carga) e controle de qualidade de dados em um ambiente de big data.

## Estrutura do Projeto
O repositório é organizado em várias pastas para facilitar o desenvolvimento e manutenção:

- **src/**: Código-fonte do pipeline, incluindo scripts PySpark e Python para a implementação do ETL.
- **tests/**: Scripts de testes automatizados que validam a qualidade e consistência dos dados.
- **data/**: Conjunto de dados usado no projeto para testes e desenvolvimento.
- **docs/**: Documentação adicional sobre o projeto, como arquitetura, descrição de funcionalidades e dependências.

## Tecnologias Utilizadas
- **Python**: Linguagem principal para os scripts e automação.
- **PySpark**: Framework para processamento de dados em larga escala.
- **SQL (BigQuery)**: Para manipulação e consultas aos dados.
- **Git/GitHub**: Controle de versão e repositório de código.
- **pytest**: Framework de testes para automatizar validações de QA/QC.

## Funcionalidades Principais
- **Pipeline ETL**: Extração, Transformação e Carga dos dados, com operações de limpeza e agregação.
- **Automação de Testes**: Validação automática da integridade e qualidade dos dados.
- **Controle de Qualidade**: Regras de QA/QC implementadas para garantir a consistência dos dados ao longo do pipeline.

## Como Executar o Projeto
1. Clone o repositório:
   ```bash
   git clone https://github.com/seu-usuario/bigdata-pipeline-project.git
